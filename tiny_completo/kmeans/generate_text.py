from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import hashlib
from pathlib import Path
import json
import re
from typing import List, Dict, Optional, Tuple

# --- CONFIGURACI√ìN MEJORADA ---
class Config:
    MODEL_PATH = "./models/fine_tuned_gpt_neo"
    GENERATED_PATH = Path("generated_problems")
    CACHE_PATH = Path("data/errors_cache.jsonl")

    # CONFIGURACI√ìN M√ÅS ESTRICTA para GPT-Neo
    GENERATION_CONFIG = {
        "temperature": 0.3,           # üîΩ M√ÅS BAJO = m√°s determinista
        "top_p": 0.85,
        "top_k": 40,                  # ‚úÖ A√±adir top_k
        "max_new_tokens": 300,        # üîΩ Reducido para m√°s enfoque
        "do_sample": True,
        "repetition_penalty": 1.3,    # üîΩ M√°s alto para menos repeticiones
        "num_beams": 1,               # B√∫squeda greedy para m√°s consistencia
    }

# --- INICIALIZACI√ìN ---
def initialize_model():
    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        Config.MODEL_PATH,
        torch_dtype=torch.float32,
    )
    model.to("cpu")
    
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=-1,
        **Config.GENERATION_CONFIG
    )
    return tokenizer, model, generator

def check_model_loading():
    model_path = Path(Config.MODEL_PATH)
    if not model_path.exists():
        print("ERROR: No se encuentra el modelo fine-tuned")
        return False
    print("‚úÖ Modelo GPT-Neo fine-tuned encontrado")
    return True

# --- CACHE (igual) ---
def load_cache() -> List[Dict]:
    cache = []
    if Config.CACHE_PATH.exists():
        with open(Config.CACHE_PATH, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    cache.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    print(f"üìä Cache cargado: {len(cache)} problemas")
    return cache

def save_cache(cache_data: List[Dict]) -> None:
    Config.CACHE_PATH.parent.mkdir(exist_ok=True, parents=True)
    with open(Config.CACHE_PATH, "w", encoding="utf-8") as f:
        for item in cache_data:
            f.write(json.dumps(item) + "\n")

def normalize_problem_text(text: str) -> str:
    text = re.sub(r'#.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.lower()

def is_problem_unique(problem_text: str, cache: List[Dict]) -> bool:
    normalized_new = normalize_problem_text(problem_text)
    for item in cache:
        if 'problem' in item:
            normalized_existing = normalize_problem_text(item['problem'])
            if normalized_new == normalized_existing:
                return False
    return True

# --- VALIDACI√ìN M√ÅS FLEXIBLE ---
def validate_problem_structure(problem_text: str) -> bool:
    """Validaci√≥n m√°s flexible para GPT-Neo"""
    lines = [l.strip() for l in problem_text.split('\n') if l.strip()]
    
    # Verificar secciones principales (case insensitive)
    sections_found = {
        'variables': any('variables' in l.lower() for l in lines),
        'minimize': any('minimize' in l.lower() for l in lines),
        'constraints': any('constraints' in l.lower() for l in lines),
        'end': any(l.lower() == 'end' for l in lines)
    }
    
    if not all(sections_found.values()):
        print(f"‚ùå Faltan secciones: {[k for k,v in sections_found.items() if not v]}")
        return False

    # Buscar variables (patr√≥n m√°s flexible)
    variables_found = any(
        re.search(r'x\d+\s+in\s*\[', l, re.IGNORECASE) and ';' in l 
        for l in lines
    )
    if not variables_found:
        print("‚ùå No se encontraron variables v√°lidas")
        return False

    # Buscar al menos una restricci√≥n
    constraints_found = any(
        any(op in l for op in ['<=', '>=', '==', '=']) and ';' in l
        for l in lines
    )
    if not constraints_found:
        print("‚ùå No se encontraron restricciones v√°lidas")
        return False

    print("‚úÖ Estructura v√°lida")
    return True

def cut_at_first_end(text: str) -> str:
    pattern = re.compile(r"(.*?\bend\b)", re.DOTALL | re.IGNORECASE)
    m = pattern.search(text)
    return m.group(1).strip() if m else text.strip()

# --- GENERACI√ìN MEJORADA ---
def generate_with_model(difficulty: str) -> str:
    """Generaci√≥n con prompt m√°s espec√≠fico"""
    prompt = f"""Generate a {difficulty} linear optimization problem.

FORMAT:
Variables
x1 in [lower, upper];
x2 in [lower, upper];

Minimize
linear_expression;

Constraints
constraint1;
constraint2;

end

Example:
Variables
x1 in [0, 10];
x2 in [1, 5];

Minimize
3*x1 + 2*x2;

Constraints
x1 + x2 <= 8;
x1 >= 2;

end

Generate a new {difficulty} problem:
"""
    
    tokenizer, model, generator = initialize_model()
    
    try:
        response = generator(
            prompt,
            max_new_tokens=Config.GENERATION_CONFIG["max_new_tokens"],
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id,
            return_full_text=False  # ‚úÖ Solo el texto generado, no el prompt
        )
        
        generated_text = response[0]['generated_text'].strip()
        return cut_at_first_end(generated_text)
        
    except Exception as e:
        print(f"‚ùå Error en generaci√≥n: {e}")
        return ""

def postprocess_problem(raw_problem: str) -> str:
    """Post-procesamiento m√°s robusto"""
    if not raw_problem:
        return ""
        
    # Limpiar comentarios y espacios
    text = re.sub(r'#.*$', '', raw_problem, flags=re.MULTILINE)
    text = re.sub(r'//.*$', '', text, flags=re.MULTILINE)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    
    if not lines:
        return ""
        
    # Reconstruir con formato consistente
    sections = []
    current_section = []
    
    for line in lines:
        lower_line = line.lower()
        
        if any(section in lower_line for section in ['variables', 'minimize', 'constraints']):
            if current_section:
                sections.append("\n".join(current_section))
            current_section = [line]
        elif lower_line == 'end':
            if current_section:
                sections.append("\n".join(current_section))
            sections.append('end')
            break
        else:
            # Asegurar que las l√≠neas terminen con ;
            if not line.endswith(';') and not any(word in lower_line for word in ['variables', 'minimize', 'constraints', 'end']):
                line += ';'
            current_section.append(line)
    
    if current_section and 'end' not in sections:
        sections.append("\n".join(current_section))
        sections.append('end')
    
    return "\n".join(sections)

def save_problem(problem_text: str, difficulty: str = "easy") -> Path:
    Config.GENERATED_PATH.mkdir(exist_ok=True, parents=True)
    difficulty_path = Config.GENERATED_PATH / difficulty
    difficulty_path.mkdir(exist_ok=True, parents=True)
    h = hashlib.sha256(problem_text.encode("utf-8")).hexdigest()[:10]
    filename = difficulty_path / f"problem_{difficulty}_{h}.bch"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(problem_text)
    return filename

def generate_new_problem(difficulty: str = "easy") -> Tuple[Optional[str], Optional[Path]]:
    print(f"\nüéØ Generando problema: {difficulty}")
    cache = load_cache()
    
    for attempt in range(15):
        print(f"üîÑ Intento {attempt + 1}/15")
        generated = generate_with_model(difficulty)
        
        if not generated:
            print("‚ùå Generaci√≥n fall√≥, reintentando...")
            continue
            
        processed = postprocess_problem(generated)
        
        if validate_problem_structure(processed) and is_problem_unique(processed, cache):
            print("‚úÖ Problema v√°lido y √∫nico generado!")
            cache.append({"difficulty": difficulty, "problem": processed})
            save_cache(cache)
            saved_path = save_problem(processed, difficulty)
            return processed, saved_path
        else:
            print("‚ùå Problema inv√°lido o duplicado, reintentando...")
    
    print("‚ùå No se pudo generar un problema v√°lido despu√©s de 15 intentos")
    return None, None

# --- EJECUCI√ìN ---
if __name__ == "__main__":
    if not check_model_loading():
        exit(1)
        
    difficulty = input("Selecciona dificultad (easy/medium/hard): ").strip().lower() or "easy"
    problem, saved_path = generate_new_problem(difficulty)
    
    if problem:
        print("\n" + "="*50)
        print("üéâ PROBLEMA GENERADO EXITOSAMENTE")
        print("="*50)
        print(problem)
        print(f"\nüíæ Guardado en: {saved_path}")
    else:
        print("\nüòû No se pudo generar un problema v√°lido")
